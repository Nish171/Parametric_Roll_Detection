{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin\")   # Run only if necessary\n",
    "\n",
    "from src.data.Dataset import Dataset as DS\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generator\n",
    "\n",
    "n = 4\n",
    "m = 2\n",
    "hop = 0.1\n",
    "input_dim   = 30 * n\n",
    "pred_dim    = 30 * m\n",
    "shift       = pred_dim\n",
    "batch_size  = 16\n",
    "Data        = DS(\n",
    "                input_dim, \n",
    "                pred_dim, \n",
    "                shift, \n",
    "                skip=0.25, \n",
    "                hop=hop, \n",
    "                batch_size=batch_size,\n",
    "                in_cols=['roll','heave','pitch'],\n",
    "                classification=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 480, 3), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 1, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.Train.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: [480, 3]\n",
      "Output dim: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = Data.xshape\n",
    "OUTPUT_DIM = Data.yshape\n",
    "\n",
    "print('Input dim:', INPUT_DIM)\n",
    "print('Output dim:', OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "# Classification loss\n",
    "Bin_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Metrics\n",
    "# Classification metrics\n",
    "Bin_accuracy = tf.keras.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm = tf.keras.layers.Normalization(input_shape=INPUT_DIM, axis=-1)\n",
    "# x_ds = Data.Train.map(lambda x, y: x)\n",
    "# norm.adapt(x_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model here\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM),\n",
    "    tf.keras.layers.Dense(64, activation='tanh'),\n",
    "    tf.keras.layers.Dense(64, activation='tanh'),\n",
    "    tf.keras.layers.Dense(32, activation='tanh'),\n",
    "    tf.keras.layers.Dense(OUTPUT_DIM[-1], activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 1440)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                92224     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,497\n",
      "Trainable params: 98,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "loss = Bin_cross_entropy\n",
    "metrics = [Bin_accuracy]\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print([int(11>10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "TypeError: '>' not supported between instances of 'str' and 'float'\nTraceback (most recent call last):\n\n  File \"c:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\nimee\\OneDrive\\Documents\\DDP\\Parametric_Roll_Detection\\src\\data\\Dataset.py\", line 204, in callable_gen\n    y = [int(max(y) > self.roll_thres)]\n\nTypeError: '>' not supported between instances of 'str' and 'float'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check if model is working and the output shape\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m Data\u001b[38;5;241m.\u001b[39mTrain\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:836\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    835\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:819\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 819\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2922\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2920\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   2921\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2922\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   2924\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: TypeError: '>' not supported between instances of 'str' and 'float'\nTraceback (most recent call last):\n\n  File \"c:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\users\\nimee\\onedrive\\documents\\ddp\\ddp\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\nimee\\OneDrive\\Documents\\DDP\\Parametric_Roll_Detection\\src\\data\\Dataset.py\", line 204, in callable_gen\n    y = [int(max(y) > self.roll_thres)]\n\nTypeError: '>' not supported between instances of 'str' and 'float'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# Check if model is working and the output shape\n",
    "\n",
    "for x, y in Data.Train.take(1):\n",
    "    out = model(x)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the required callbacks\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=15,\n",
    "                                                    mode='min')\n",
    "\n",
    "checkpoint_filepath = 'models/Classification/01/model'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                              patience=5, min_lr=0.000001)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"models/Classification/01/metrics\", update_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "model.fit(Data.Train, epochs=100, validation_data=Data.Val, callbacks=[early_stopping, model_checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "# Uncomment to save model\n",
    "\n",
    "# model_save_dir = 'models/LSTM/03r/model'\n",
    "\n",
    "# model.save(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING SAVED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model_dir = 'models/Classification/01/model'\n",
    "\n",
    "model = tf.keras.models.load_model(model_dir)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "\n",
    "val_performance = model.evaluate(Data.Val)\n",
    "test_performance = model.evaluate(Data.Test)\n",
    "\n",
    "print('Validation MSE_loss: {:.2f}'.format(val_performance[0]))\n",
    "print('Validation MAPE:     {:.2f}'.format(val_performance[1]))\n",
    "print('Validation MAE:      {:.2f}'.format(val_performance[2]))\n",
    "print('Test MSE_loss:       {:.2f}'.format(test_performance[0]))\n",
    "print('Test MAPE:           {:.2f}'.format(test_performance[1]))\n",
    "print('Test MAE:            {:.2f}'.format(test_performance[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.plot_example(50, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNING RATE OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def lr_optimizer(model, window, patience=2):\n",
    "\n",
    "  lr_scheduler = tf.keras.callbacks.LearningRateScheduler(\n",
    "      lambda epoch: 1e-7 * 10**(epoch / 4))\n",
    "\n",
    "  model.compile(loss=MSE_loss,\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[MAPE, MAE])\n",
    "\n",
    "  history = model.fit(window.Train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.Val,\n",
    "                      callbacks=[lr_scheduler])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = lr_optimizer(oneshot_rnn_model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = 1e-5 * (10 ** (np.arange(MAX_EPOCHS) / 4))\n",
    "plt.semilogx(lrs, history.history['loss'])\n",
    "plt.axis([1e-5, 1e-3, 0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c200e79e69ee85ae55c6336c5e814a93afe2e8f0b92a107a93399dcb16a13be9"
  },
  "kernelspec": {
   "display_name": "ddp",
   "language": "python",
   "name": "ddp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
